{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import dateutil\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import urllib3\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
      ],
      "metadata": {
        "id": "9bZVJg0sYE4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = 'wHSk4qAJFso7oBI9YAT7DrkDNunyU1T8'"
      ],
      "metadata": {
        "id": "qZ7nnPC6YGZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_RETRIES = 5\n",
        "INITIAL_DELAY = 1\n",
        "\n",
        "def send_request(date):\n",
        "    base_url = 'https://api.nytimes.com/svc/news/v3/content/nyt/business/'\n",
        "    url = base_url + '/' + date[0] + '/' + date[1] + '.json?api-key=' + API_KEY\n",
        "    retries = 0\n",
        "    while retries < MAX_RETRIES:\n",
        "        try:\n",
        "            response = requests.get(url, verify=False).json()\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "            retries += 1\n",
        "            delay = INITIAL_DELAY * (2 ** retries)\n",
        "            print(f\"Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay)\n",
        "    return None\n",
        "\n",
        "\n",
        "def is_valid(article, date):\n",
        "    is_in_range = date > start and date < end\n",
        "    has_headline = type(article['headline']) == dict and 'main' in article['headline'].keys()\n",
        "    return is_in_range and has_headline\n",
        "\n",
        "\n",
        "def parse_response(response):\n",
        "    data = {'headline': [],\n",
        "        'date': [],\n",
        "        'web_url': [],\n",
        "        'doc_type': [],\n",
        "        'lead_paragraph': [],\n",
        "        'material_type': [],\n",
        "        'author': [],\n",
        "        'section': [],\n",
        "        'subsection': [],\n",
        "        'keywords': []}\n",
        "\n",
        "    articles = response['response']['docs']\n",
        "    for article in articles:\n",
        "        date = dateutil.parser.parse(article['pub_date']).date()\n",
        "        if is_valid(article, date):\n",
        "            data['date'].append(date)\n",
        "            data['headline'].append(article['headline']['main'])\n",
        "            if 'section_name' in article:\n",
        "                data['section'].append(article['section_name'])\n",
        "            else:\n",
        "                data['section'].append(None)\n",
        "            if 'lead_paragraph' in article:\n",
        "                data['lead_paragraph'].append(article['lead_paragraph'])\n",
        "            else:\n",
        "                data['lead_paragraph'].append(None)\n",
        "            if 'web_url' in article:\n",
        "                data['web_url'].append(article['web_url'])\n",
        "            else:\n",
        "                data['web_url'].append(None)\n",
        "            if 'subsection_name' in article:\n",
        "                data['subsection'].append(article['subsection_name'])\n",
        "            else:\n",
        "                data['subsection'].append(None)\n",
        "            if 'byline' in article:\n",
        "                data['author'].append(article['byline']['original'])\n",
        "            else:\n",
        "                data['author'].append(None)\n",
        "            data['doc_type'].append(article['document_type'])\n",
        "            if 'type_of_material' in article:\n",
        "                data['material_type'].append(article['type_of_material'])\n",
        "            else:\n",
        "                data['material_type'].append(None)\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            data['keywords'].append(keywords)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def get_data(dates):\n",
        "    total = 0\n",
        "    print('Date range: ' + str(dates[0]) + ' to ' + str(dates[-1]))\n",
        "    if not os.path.exists('headlines'):\n",
        "        os.mkdir('headlines')\n",
        "    for date in dates:\n",
        "        print('Working on ' + str(date) + '...')\n",
        "        csv_path = 'headlines/' + date[0] + '-' + date[1] + '.csv'\n",
        "        if not os.path.exists(csv_path): # If we don't already have this month\n",
        "            response = send_request(date)\n",
        "            if response is not None:\n",
        "                df = parse_response(response)\n",
        "                total += len(df)\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                print('Saving ' + csv_path + '...')\n",
        "    print('Number of articles collected: ' + str(total))"
      ],
      "metadata": {
        "id": "PfHn8gSiYc3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "end = datetime.date(2022, 9, 29)\n",
        "start = datetime.date(2021, 9, 30)"
      ],
      "metadata": {
        "id": "_pzVpXJTYnkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "months = [x.split(' ') for x in pd.date_range(start, end, freq='MS').strftime(\"%Y %-m\").tolist()]\n"
      ],
      "metadata": {
        "id": "4QtZAD9vYtwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_data(months)"
      ],
      "metadata": {
        "id": "59-soebfYHKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "os.chdir(\"/content/headlines\") ## use Google Colab"
      ],
      "metadata": {
        "id": "PUKJW8XOdfs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]"
      ],
      "metadata": {
        "id": "1np0LTZZdjrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#combine in a single file\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
        "#export to csv\n",
        "combined_csv.to_csv( \"combined_csv2.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "Riip61_EdpMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}